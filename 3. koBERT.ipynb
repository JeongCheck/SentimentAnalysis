{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.optim import Optimizer\n",
    "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, random_split\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "from transformers import AdamW\n",
    "from transformers.optimization import get_cosine_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model\n",
    "from kobert.utils import get_tokenizer\n",
    "from kobert.pytorch_kobert import get_pytorch_kobert_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n",
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "bertmodel, vocab = get_pytorch_kobert_model() #get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Get the GPU device name.\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "# The device name should look like the following:\n",
    "if device_name == '/device:GPU:0':\n",
    "    print('Found GPU at: {}'.format(device_name))\n",
    "else:\n",
    "    raise SystemError('GPU device not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2070\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = nlp.data.TSVDataset(\"nsmc/ratings_train.txt\", field_indices=[1,2], num_discard_samples=1)\n",
    "dataset_test = nlp.data.TSVDataset(\"nsmc/ratings_test.txt\", field_indices=[1,2], num_discard_samples=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gluonnlp.data.TSVDataset 로 로드\n",
    "gluonnlp.data.BERTSentenceTransform 으로 transform => 토크나이징이 진행된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using cached model\n"
     ]
    }
   ],
   "source": [
    "#토크나이저\n",
    "tokenizer = get_tokenizer()\n",
    "tok = nlp.data.BERTSPTokenizer(tokenizer, vocab, lower=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = nlp.data.BERTSentenceTransform(tokenizer=tok,\n",
    "                                           max_seq_length=140,\n",
    "                                           pad=True,\n",
    "                                           pair=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([   2, 2348,    3,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "          1,    1,    1,    1,    1,    1,    1,    1], dtype=int32), array(3, dtype=int32), array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "       0, 0, 0, 0, 0, 0, 0, 0], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "from_string = transform('별 반개도 아깝다 욕나온다 이응경 길용우 연기생활이몇년인지..정말 발로해도 그것보단 낫겟다 납치.감금만반복반복..이드라마는 가족도없다 연기못하는사람만모엿네')\n",
    "print(from_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BERTDataset(Dataset):\n",
    "    def __init__(self, dataset, sent_idx, label_idx, bert_tokenizer, max_len,\n",
    "                 pad, pair):\n",
    "        transform = nlp.data.BERTSentenceTransform(\n",
    "            bert_tokenizer, max_seq_length=max_len, pad=pad, pair=pair)\n",
    "\n",
    "        self.sentences = [transform([i[sent_idx]]) for i in dataset]\n",
    "        self.labels = [np.int32(i[label_idx]) for i in dataset]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return (self.sentences[i] + (self.labels[i], ))\n",
    "\n",
    "    def __len__(self):\n",
    "        return (len(self.labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting parameters\n",
    "max_len = 128 # 해당 길이를 초과하는 단어에 대해선 bert가 학습하지 않음\n",
    "batch_size = 32\n",
    "warmup_ratio = 0.1\n",
    "num_epochs = 5\n",
    "max_grad_norm = 1\n",
    "log_interval = 200\n",
    "learning_rate =  5e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "data_train = BERTDataset(dataset_train, 0, 1, tok, max_len, True, False)\n",
    "data_test = BERTDataset(dataset_test, 0, 1, tok, max_len, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(data_train, batch_size=batch_size, num_workers=5)\n",
    "test_dataloader = torch.utils.data.DataLoader(data_test, batch_size=batch_size, num_workers=5)\n",
    "#파이토치 텐서로 변환해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_size = 768,\n",
    "                 num_classes = 5, # softmax 사용 <- binary일 경우는 2\n",
    "                 dr_rate=None,\n",
    "                 params=None):\n",
    "        super(BERTClassifier, self).__init__()\n",
    "        self.bert = bert\n",
    "        self.dr_rate = dr_rate\n",
    "                 \n",
    "        self.classifier = nn.Linear(hidden_size , num_classes)\n",
    "        if dr_rate:\n",
    "            self.dropout = nn.Dropout(p=dr_rate)\n",
    "    \n",
    "    def gen_attention_mask(self, token_ids, valid_length):\n",
    "        attention_mask = torch.zeros_like(token_ids)\n",
    "        for i, v in enumerate(valid_length):\n",
    "            attention_mask[i][:v] = 1\n",
    "        return attention_mask.float()\n",
    "\n",
    "    def forward(self, token_ids, valid_length, segment_ids):\n",
    "        attention_mask = self.gen_attention_mask(token_ids, valid_length)\n",
    "        \n",
    "        _, pooler = self.bert(input_ids = token_ids, token_type_ids = segment_ids.long(), attention_mask = attention_mask.float().to(token_ids.device))\n",
    "        if self.dr_rate:\n",
    "            out = self.dropout(pooler)\n",
    "        return self.classifier(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BERTClassifier(bertmodel, dr_rate=0.5).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare optimizer and schedule (linear warmup and decay)\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=learning_rate)\n",
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = len(train_dataloader) * num_epochs\n",
    "warmup_step = int(t_total * warmup_ratio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 평가 지표인 accuracy 계산 -> 얼마나 타겟값을 많이 맞추었는가\n",
    "def calc_accuracy(X,Y):\n",
    "    max_vals, max_indices = torch.max(X, 1)\n",
    "    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n",
    "    return train_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiffel-dj57/anaconda3/envs/aiffel/lib/python3.7/site-packages/ipykernel_launcher.py:8: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13d3b542c434f32ab9bf9fa06477e83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 batch id 1 loss 1.629084825515747 train acc 0.1875\n",
      "epoch 1 batch id 201 loss 0.9285391569137573 train acc 0.4510261194029851\n",
      "epoch 1 batch id 401 loss 0.5567103028297424 train acc 0.5678771820448878\n",
      "epoch 1 batch id 601 loss 0.40651535987854004 train acc 0.646734608985025\n",
      "epoch 1 batch id 801 loss 0.4441331624984741 train acc 0.6923767166042447\n",
      "epoch 1 batch id 1001 loss 0.38147395849227905 train acc 0.7232455044955045\n",
      "epoch 1 batch id 1201 loss 0.6003241539001465 train acc 0.7437552039966694\n",
      "epoch 1 batch id 1401 loss 0.2666613459587097 train acc 0.7597028907922913\n",
      "epoch 1 batch id 1601 loss 0.5532408952713013 train acc 0.7708853841349157\n",
      "epoch 1 batch id 1801 loss 0.5103179216384888 train acc 0.7796189616879511\n",
      "epoch 1 batch id 2001 loss 0.2986183762550354 train acc 0.7872626186906547\n",
      "epoch 1 batch id 2201 loss 0.5946784019470215 train acc 0.7937443207632894\n",
      "epoch 1 batch id 2401 loss 0.26377755403518677 train acc 0.799003019575177\n",
      "epoch 1 batch id 2601 loss 0.34668296575546265 train acc 0.8032487504805844\n",
      "epoch 1 batch id 2801 loss 0.32951921224594116 train acc 0.8072898072117101\n",
      "epoch 1 batch id 3001 loss 0.3602871298789978 train acc 0.810958847050983\n",
      "epoch 1 batch id 3201 loss 0.21986517310142517 train acc 0.8142475007810059\n",
      "epoch 1 batch id 3401 loss 0.4053760766983032 train acc 0.8172044986768597\n",
      "epoch 1 batch id 3601 loss 0.19770517945289612 train acc 0.8201194112746459\n",
      "epoch 1 batch id 3801 loss 0.38209444284439087 train acc 0.822653578005788\n",
      "epoch 1 batch id 4001 loss 0.23085825145244598 train acc 0.8253874031492127\n",
      "epoch 1 batch id 4201 loss 0.2587478756904602 train acc 0.8276600809331112\n",
      "epoch 1 batch id 4401 loss 0.38376590609550476 train acc 0.8298256078164054\n",
      "epoch 1 batch id 4601 loss 0.2876667380332947 train acc 0.831897956965877\n",
      "epoch 1 train acc 0.8328311646757679\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4217e0a9c0fc4c418fcc2b84fd9eef71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 batch id 1 loss 0.30649450421333313 train acc 0.8125\n",
      "epoch 2 batch id 201 loss 0.26623180508613586 train acc 0.878731343283582\n",
      "epoch 2 batch id 401 loss 0.28749722242355347 train acc 0.8794420199501247\n",
      "epoch 2 batch id 601 loss 0.21214497089385986 train acc 0.8816035773710482\n",
      "epoch 2 batch id 801 loss 0.4154081642627716 train acc 0.88229556803995\n",
      "epoch 2 batch id 1001 loss 0.34163832664489746 train acc 0.8852709790209791\n",
      "epoch 2 batch id 1201 loss 0.45722338557243347 train acc 0.8873334721065779\n",
      "epoch 2 batch id 1401 loss 0.348471462726593 train acc 0.8892978229835832\n",
      "epoch 2 batch id 1601 loss 0.432079553604126 train acc 0.8902638975640225\n",
      "epoch 2 batch id 1801 loss 0.3545224070549011 train acc 0.8912583287062743\n",
      "epoch 2 batch id 2001 loss 0.2411753386259079 train acc 0.8916479260369815\n",
      "epoch 2 batch id 2201 loss 0.35148435831069946 train acc 0.8927476147205815\n",
      "epoch 2 batch id 2401 loss 0.23675966262817383 train acc 0.8938072678050812\n",
      "epoch 2 batch id 2601 loss 0.13219135999679565 train acc 0.8951244713571703\n",
      "epoch 2 batch id 2801 loss 0.0976468026638031 train acc 0.8959969653695109\n",
      "epoch 2 batch id 3001 loss 0.22043326497077942 train acc 0.8969301899366877\n",
      "epoch 2 batch id 3201 loss 0.33166587352752686 train acc 0.897922524211184\n",
      "epoch 2 batch id 3401 loss 0.37205731868743896 train acc 0.8990554248750368\n",
      "epoch 2 batch id 3601 loss 0.08617563545703888 train acc 0.8999409886142739\n",
      "epoch 2 batch id 3801 loss 0.36315134167671204 train acc 0.9006511444356748\n",
      "epoch 2 batch id 4001 loss 0.13193650543689728 train acc 0.9017120719820045\n",
      "epoch 2 batch id 4201 loss 0.18520468473434448 train acc 0.9024562604141871\n",
      "epoch 2 batch id 4401 loss 0.30598658323287964 train acc 0.9032038173142468\n",
      "epoch 2 batch id 4601 loss 0.2175430953502655 train acc 0.9041512714627254\n",
      "epoch 2 train acc 0.9044835217576792\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96bc75d404e416096bc33fea6192f3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 batch id 1 loss 0.45624464750289917 train acc 0.78125\n",
      "epoch 3 batch id 201 loss 0.14674066007137299 train acc 0.9225746268656716\n",
      "epoch 3 batch id 401 loss 0.22530721127986908 train acc 0.9212125935162094\n",
      "epoch 3 batch id 601 loss 0.12346389889717102 train acc 0.9222129783693843\n",
      "epoch 3 batch id 801 loss 0.25826695561408997 train acc 0.9229088639200999\n",
      "epoch 3 batch id 1001 loss 0.28246068954467773 train acc 0.9240134865134865\n",
      "epoch 3 batch id 1201 loss 0.4905385971069336 train acc 0.9260772273105745\n",
      "epoch 3 batch id 1401 loss 0.357593297958374 train acc 0.9274848322626695\n",
      "epoch 3 batch id 1601 loss 0.30392494797706604 train acc 0.928540755777639\n",
      "epoch 3 batch id 1801 loss 0.14707671105861664 train acc 0.929865352581899\n",
      "epoch 3 batch id 2001 loss 0.0699525773525238 train acc 0.9305347326336831\n",
      "epoch 3 batch id 2201 loss 0.3827323913574219 train acc 0.931792367105861\n",
      "epoch 3 batch id 2401 loss 0.18374399840831757 train acc 0.9327233444398167\n",
      "epoch 3 batch id 2601 loss 0.15719646215438843 train acc 0.9335231641676278\n",
      "epoch 3 batch id 2801 loss 0.04809983819723129 train acc 0.9344542127811496\n",
      "epoch 3 batch id 3001 loss 0.10341425240039825 train acc 0.9350528990336554\n",
      "epoch 3 batch id 3201 loss 0.2093210518360138 train acc 0.9357232114964074\n",
      "epoch 3 batch id 3401 loss 0.18787695467472076 train acc 0.9366454719200236\n",
      "epoch 3 batch id 3601 loss 0.04159471392631531 train acc 0.9375\n",
      "epoch 3 batch id 3801 loss 0.20532822608947754 train acc 0.9381166140489345\n",
      "epoch 3 batch id 4001 loss 0.032129041850566864 train acc 0.938968382904274\n",
      "epoch 3 batch id 4201 loss 0.0191134512424469 train acc 0.9396200309450131\n",
      "epoch 3 batch id 4401 loss 0.1814500093460083 train acc 0.9400065326062259\n",
      "epoch 3 batch id 4601 loss 0.1281290352344513 train acc 0.9405835687893936\n",
      "epoch 3 train acc 0.9408863054607508\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b144afe07af4c43adf0cfa25aa4167d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 batch id 1 loss 0.4996744990348816 train acc 0.84375\n",
      "epoch 4 batch id 201 loss 0.08056577295064926 train acc 0.9499378109452736\n",
      "epoch 4 batch id 401 loss 0.09467220306396484 train acc 0.952540523690773\n",
      "epoch 4 batch id 601 loss 0.17174412310123444 train acc 0.9550228785357737\n",
      "epoch 4 batch id 801 loss 0.033657558262348175 train acc 0.9561875780274657\n",
      "epoch 4 batch id 1001 loss 0.0381745770573616 train acc 0.9570117382617382\n",
      "epoch 4 batch id 1201 loss 0.470498263835907 train acc 0.9577955870108243\n",
      "epoch 4 batch id 1401 loss 0.19677937030792236 train acc 0.9585340827980015\n",
      "epoch 4 batch id 1601 loss 0.16007354855537415 train acc 0.9589709556527171\n",
      "epoch 4 batch id 1801 loss 0.1742366999387741 train acc 0.9596578289838978\n",
      "epoch 4 batch id 2001 loss 0.012142017483711243 train acc 0.960410419790105\n",
      "epoch 4 batch id 2201 loss 0.3076639771461487 train acc 0.9610404361653794\n",
      "epoch 4 batch id 2401 loss 0.1491834670305252 train acc 0.9614743856726364\n",
      "epoch 4 batch id 2601 loss 0.007227182388305664 train acc 0.9620218185313341\n",
      "epoch 4 batch id 2801 loss 0.11142023652791977 train acc 0.9623683505890753\n",
      "epoch 4 batch id 3001 loss 0.02541983872652054 train acc 0.9625541486171276\n",
      "epoch 4 batch id 3201 loss 0.019139371812343597 train acc 0.9630779443923774\n",
      "epoch 4 batch id 3401 loss 0.16056521236896515 train acc 0.9636687738900324\n",
      "epoch 4 batch id 3601 loss 0.009442538022994995 train acc 0.9643067897806165\n",
      "epoch 4 batch id 3801 loss 0.11270545423030853 train acc 0.9646967903183373\n",
      "epoch 4 batch id 4001 loss 0.005232751369476318 train acc 0.9651805798550362\n",
      "epoch 4 batch id 4201 loss 0.009567320346832275 train acc 0.9652984408474172\n",
      "epoch 4 batch id 4401 loss 0.16047845780849457 train acc 0.9655547034764826\n",
      "epoch 4 batch id 4601 loss 0.033566027879714966 train acc 0.9660535753097153\n",
      "epoch 4 train acc 0.9663369240614335\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5098ab99ba424326b4d4e5278bc19cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4688 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 batch id 1 loss 0.7392314672470093 train acc 0.84375\n",
      "epoch 5 batch id 201 loss 0.01995018869638443 train acc 0.9731032338308457\n",
      "epoch 5 batch id 401 loss 0.07334630936384201 train acc 0.975218204488778\n",
      "epoch 5 batch id 601 loss 0.11765656620264053 train acc 0.9759255407653911\n",
      "epoch 5 batch id 801 loss 0.010485641658306122 train acc 0.975187265917603\n",
      "epoch 5 batch id 1001 loss 0.11940737813711166 train acc 0.9750561938061938\n",
      "epoch 5 batch id 1201 loss 0.3990212082862854 train acc 0.975541215653622\n",
      "epoch 5 batch id 1401 loss 0.17361751198768616 train acc 0.9757985367594575\n",
      "epoch 5 batch id 1601 loss 0.12341782450675964 train acc 0.9759134915677702\n",
      "epoch 5 batch id 1801 loss 0.014503002166748047 train acc 0.9763672959466962\n",
      "epoch 5 batch id 2001 loss 0.13301394879817963 train acc 0.9766991504247876\n",
      "epoch 5 batch id 2201 loss 0.2023618519306183 train acc 0.9771694684234439\n",
      "epoch 5 batch id 2401 loss 0.12459675222635269 train acc 0.9774703248646397\n",
      "epoch 5 batch id 2601 loss 0.003336489200592041 train acc 0.9774846212995002\n",
      "epoch 5 batch id 2801 loss 0.002469658851623535 train acc 0.977675383791503\n",
      "epoch 5 batch id 3001 loss 0.18101385235786438 train acc 0.9776220426524492\n",
      "epoch 5 batch id 3201 loss 0.006195038557052612 train acc 0.9778877694470478\n",
      "epoch 5 batch id 3401 loss 0.2799205183982849 train acc 0.9781865627756542\n",
      "epoch 5 batch id 3601 loss 0.15175577998161316 train acc 0.9782438905859483\n",
      "epoch 5 batch id 3801 loss 0.12253919243812561 train acc 0.9784349513285977\n",
      "epoch 5 batch id 4001 loss 0.006978273391723633 train acc 0.9786693951512122\n",
      "epoch 5 batch id 4201 loss 0.013203330338001251 train acc 0.9786211616281838\n",
      "epoch 5 batch id 4401 loss 0.08610711246728897 train acc 0.9787406271301977\n",
      "epoch 5 batch id 4601 loss 0.07009632885456085 train acc 0.9788089545750923\n",
      "epoch 5 train acc 0.9789422461604096\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "# 모델 학습 시작\n",
    "for e in range(num_epochs):\n",
    "    train_acc = 0.0\n",
    "    test_acc = 0.0\n",
    "    model.train()\n",
    "    for batch_id, (token_ids, valid_length, segment_ids, label) in enumerate(tqdm_notebook(train_dataloader)):\n",
    "        optimizer.zero_grad()\n",
    "        token_ids = token_ids.long().to(device)\n",
    "        segment_ids = segment_ids.long().to(device)\n",
    "        valid_length= valid_length\n",
    "        label = label.long().to(device)\n",
    "        out = model(token_ids, valid_length, segment_ids)\n",
    "        loss = loss_fn(out, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm) # gradient clipping\n",
    "        optimizer.step()\n",
    "        scheduler.step()  # Update learning rate schedule\n",
    "        train_acc += calc_accuracy(out, label)\n",
    "        if batch_id % log_interval == 0:\n",
    "            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n",
    "    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiffel",
   "language": "python",
   "name": "aiffel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
